{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112354a2",
   "metadata": {
    "papermill": {
     "duration": 0.021742,
     "end_time": "2023-08-23T10:17:47.933827",
     "exception": false,
     "start_time": "2023-08-23T10:17:47.912085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1fbb52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:47.979921Z",
     "iopub.status.busy": "2023-08-23T10:17:47.979114Z",
     "iopub.status.idle": "2023-08-23T10:17:51.166450Z",
     "shell.execute_reply": "2023-08-23T10:17:51.165311Z"
    },
    "papermill": {
     "duration": 3.213809,
     "end_time": "2023-08-23T10:17:51.169389",
     "exception": false,
     "start_time": "2023-08-23T10:17:47.955580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a91a49e",
   "metadata": {
    "papermill": {
     "duration": 0.021906,
     "end_time": "2023-08-23T10:17:51.213373",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.191467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef203c5",
   "metadata": {
    "papermill": {
     "duration": 0.021519,
     "end_time": "2023-08-23T10:17:51.256599",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.235080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff5c5e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:51.302882Z",
     "iopub.status.busy": "2023-08-23T10:17:51.302168Z",
     "iopub.status.idle": "2023-08-23T10:17:51.319518Z",
     "shell.execute_reply": "2023-08-23T10:17:51.318631Z"
    },
    "papermill": {
     "duration": 0.043323,
     "end_time": "2023-08-23T10:17:51.321722",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.278399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(data, splits=0.8):\n",
    "    '''\n",
    "    Prepares the data by splitting into training, validation(if specified) and test sets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.core.frame.DataFrame\n",
    "          Dataset that needs to be splitted\n",
    "          Dates in the dataset should be an index with type as pandas.core.indexes.datetimes.DatetimeIndex \n",
    "          and needs to be sorted\n",
    "    splits : float, tuple\n",
    "             Default = 0.8; splitting data in two sets - train and test of size 0.8 and 0.2 respectively\n",
    "             float: Splits in two sets - train and test\n",
    "                    Only accepts value between range (0.0, 1.0)\n",
    "             tuple(train_size, validation_size): Splits in three sets - train, validation and test\n",
    "                                                 train_size -> Train size\n",
    "                                                 validation_size -> Validation size\n",
    "                                                 Should only contain values between range (0.0, 1.0) and \n",
    "                                                 \"train_size + test_size\" should also be between (0.0, 1.0)\n",
    "                                           \n",
    "    Returns\n",
    "    -------\n",
    "    Train, test and validation(if tuples) splits : (pandas.core.frame.DataFrame)\n",
    "    If splits is float - Train and test splits \n",
    "    If splits is a tuple - Train, validation, and test splits\n",
    "    '''\n",
    "    \n",
    "    # if data is not pandas dataframe then error\n",
    "    if not isinstance(data, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"data is not a pandas dataframe\")\n",
    "    \n",
    "    if not isinstance(data.index, pd.core.indexes.datetimes.DatetimeIndex):\n",
    "        raise TypeError(\"data does not have index in pandas DatetimeIndex format\")\n",
    "    \n",
    "    # if splits is not float or tuple then error\n",
    "    if not isinstance(splits, (float, tuple)):\n",
    "        raise TypeError(\"splits is not float or tuple\")\n",
    "\n",
    "    # check if dates are sorted or not, and if not raise exception\n",
    "    if(data.index.is_monotonic_increasing==False):\n",
    "        raise Exception(\"Dates are not sorted in the dataframe\")\n",
    "    \n",
    "    #if splits is float then perform\n",
    "    if isinstance(splits, float):\n",
    "        # check if splits is between 0.0 and 1.0\n",
    "        if(splits<=0.0 or splits>=1.0):\n",
    "            raise Exception(\"splits can be only between 0.0 and 1.0\")\n",
    "        \n",
    "        # splits in two sets - train and test and returns\n",
    "        train = data[:int(splits*(len(data)))]\n",
    "        test = data[int(splits*(len(data))):]\n",
    "        return train, test \n",
    "\n",
    "    #if splits is tuple then perform\n",
    "    if isinstance(splits, tuple):\n",
    "        train_size = splits[0]\n",
    "        test_size = splits[1]\n",
    "        # check if tuple vals contain float\n",
    "        if not isinstance(train_size, (float)):\n",
    "            raise TypeError(\"Should contain only float\")\n",
    "        if not isinstance(test_size, (float)):\n",
    "            raise TypeError(\"Should contain only float\")\n",
    "            \n",
    "        # tuple len is not equal to two then exception\n",
    "        if len(splits)!=2:\n",
    "            raise Exception(\"Split should contain only two values\")\n",
    "            \n",
    "        # check if all value in splits is between 0.0 and 1.0\n",
    "        if(train_size<=0.0 or train_size>=1.0):\n",
    "            raise Exception(\"splits can only be between 0.0 and 1.0\")\n",
    "        if(test_size<=0.0 or test_size>=1.0):\n",
    "            raise Exception(\"splits can only be between 0.0 and 1.0\")\n",
    "        \n",
    "        # check if sum of splits is between 0.0 and 1.0\n",
    "        if((train_size+test_size)<=0.0 or (train_size+test_size)>=1.0):\n",
    "            raise Exception(\"sum of values in tuples in splits should be <1.0\")\n",
    "        \n",
    "        # splits in three sets - train, validation, test and returns\n",
    "        train = data[:int(train_size*(len(data)))]\n",
    "        val = data[int(train_size*(len(data))):int((train_size+test_size)*(len(data)))]\n",
    "        test = data[int((train_size+test_size)*(len(data))):]\n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0062e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:02:50.938957Z",
     "iopub.status.busy": "2023-08-23T10:02:50.938584Z",
     "iopub.status.idle": "2023-08-23T10:02:50.944577Z",
     "shell.execute_reply": "2023-08-23T10:02:50.942939Z",
     "shell.execute_reply.started": "2023-08-23T10:02:50.938928Z"
    },
    "papermill": {
     "duration": 0.021358,
     "end_time": "2023-08-23T10:17:51.364750",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.343392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### SARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a492638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:51.410918Z",
     "iopub.status.busy": "2023-08-23T10:17:51.410209Z",
     "iopub.status.idle": "2023-08-23T10:17:51.422573Z",
     "shell.execute_reply": "2023-08-23T10:17:51.421392Z"
    },
    "papermill": {
     "duration": 0.038882,
     "end_time": "2023-08-23T10:17:51.425330",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.386448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sarima_model(train, test, val=None, **model_parameters):\n",
    "    '''\n",
    "    Creates a SARIMA model and returns predictions for the train and test data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas.core.frame.DataFrame\n",
    "            Train dataset that will be used for training the model\n",
    "    test : pandas.core.frame.DataFrame\n",
    "            Test dataset that will be used for evaluating the model\n",
    "    val : NoneType or pandas.core.frame.DataFrame\n",
    "          Default : None\n",
    "          Validation dataset that will be used for evaluating the model\n",
    "    model_parameters : kwargs\n",
    "                       Keyword argument that can be used to provide parameters to the model\n",
    "                       Can take statsmodels.tsa.statespace.sarimax.SARIMAX as parameters\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    train_preds : pandas.core.series.Series\n",
    "                  Predictions made by the model on training dataset\n",
    "    val_preds : pandas.core.series.Series\n",
    "                Predictions made by the model on validation dataset (If val is not None)\n",
    "    test_preds : pandas.core.series.Series\n",
    "                 Predictions made by the model on testing dataset\n",
    "    '''\n",
    "\n",
    "    # check if train and test are pandas dataframe\n",
    "    if not isinstance(train, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"train is not a pandas dataframe\")\n",
    "    \n",
    "    if not isinstance(test, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"test is not a pandas dataframe\")\n",
    "    \n",
    "    # If val is not pandas dataframe or not NoneType\n",
    "    if not isinstance(val, pd.core.frame.DataFrame):\n",
    "        if val is not None:\n",
    "            raise TypeError(\"val is not a pandas dataframe\")\n",
    "    \n",
    "    # model creation + fit\n",
    "    sarima_model = SARIMAX(train,  **model_parameters)\n",
    "    sarima_model_fit = sarima_model.fit()\n",
    "\n",
    "    if val is None:\n",
    "        # predictions for train and test\n",
    "        train_preds = sarima_model_fit.predict(start=1,end=len(train)-1)\n",
    "        test_preds = sarima_model_fit.predict(start=len(train), end=len(train)+len(test)-1)\n",
    "        return train_preds, test_preds\n",
    "    \n",
    "    else:\n",
    "        # predictions for train, val, test\n",
    "        train_preds = sarima_model_fit.predict(start=1,end=len(train)-1)\n",
    "        val_preds = sarima_model_fit.predict(start=len(train), end=len(train)+len(val)-1)\n",
    "        test_preds = sarima_model_fit.predict(start=len(train)+len(val), end=len(train)+len(val)+len(test)-1)\n",
    "        return train_preds, val_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd25d58",
   "metadata": {
    "papermill": {
     "duration": 0.02172,
     "end_time": "2023-08-23T10:17:51.470325",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.448605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### ETS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bea7097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:51.516764Z",
     "iopub.status.busy": "2023-08-23T10:17:51.515906Z",
     "iopub.status.idle": "2023-08-23T10:17:51.531029Z",
     "shell.execute_reply": "2023-08-23T10:17:51.530214Z"
    },
    "papermill": {
     "duration": 0.041147,
     "end_time": "2023-08-23T10:17:51.533515",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.492368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ets_model(train, test, val=None, **model_parameters):\n",
    "    '''\n",
    "    Creates a Triple Exponential Smoothing Model by calling statsmodels.tsa.holtwinters.ExponentialSmoothing and returns predictions for the train, val and test data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas.core.frame.DataFrame\n",
    "            Train dataset that will be used for training the model\n",
    "    test : pandas.core.frame.DataFrame\n",
    "           Test dataset that will be used for evaluating the model\n",
    "    val : NoneType or pandas.core.frame.DataFrame\n",
    "          Default : None\n",
    "          Validation dataset that will be used for evaluating the model\n",
    "    model_parameters: kwargs\n",
    "                      Keyword argument that can be used to provide parameters to the model\n",
    "                      Can take statsmodels.tsa.holtwinters.ExponentialSmoothing parameters and\n",
    "                      smoothing_level, smoothing_trend, smoothing_seasonal parameters of \n",
    "                      statsmodels.tsa.holtwinters.ExponentialSmoothing.fit\n",
    "                      default smoothing_level = 0.5\n",
    "                      default smoothing_trend = 0.5\n",
    "                      default smoothing seasonal = 0.5\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    train_preds : pandas.core.series.Series\n",
    "                  Predictions made by the model on training dataset\n",
    "    val_preds : pandas.core.series.Series\n",
    "                Predictions made by the model on validation dataset (If val is not None)\n",
    "    test_preds : pandas.core.series.Series\n",
    "                 Predictions made by the model on testing dataset\n",
    "    '''\n",
    "    # check if train and test are pandas dataframe\n",
    "    if not isinstance(train, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"train is not a pandas dataframe\")\n",
    "    \n",
    "    if not isinstance(test, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"test is not a pandas dataframe\")\n",
    "\n",
    "    # If val is not pandas dataframe or not NoneType\n",
    "    if not isinstance(val, pd.core.frame.DataFrame):\n",
    "        if val is not None:\n",
    "            raise TypeError(\"val is not a pandas dataframe\")\n",
    "\n",
    "    # default fit parameters\n",
    "    smoothing_level=0.5\n",
    "    smoothing_trend=0.5\n",
    "    smoothing_seasonal=0.5\n",
    "    \n",
    "    # If fit parameters are specified by user with model_parameters then removing those from model_parameters \n",
    "    #and updating default fit parameters to whatever is specified as input\n",
    "    if('smoothing_level' in model_parameters):\n",
    "        smoothing_level = model_parameters['smoothing_level']\n",
    "        model_parameters.pop('smoothing_level')\n",
    "        \n",
    "    if('smoothing_trend' in model_parameters):\n",
    "        smoothing_trend = model_parameters['smoothing_trend']\n",
    "        model_parameters.pop('smoothing_trend')\n",
    "        \n",
    "    if('smoothing_seasonal' in model_parameters):\n",
    "        smoothing_seasonal = model_parameters['smoothing_seasonal']\n",
    "        model_parameters.pop('smoothing_seasonal')\n",
    "\n",
    "    # model creation + fit\n",
    "    EXPmodel = ExponentialSmoothing(train, **model_parameters).fit(smoothing_level=smoothing_level,\n",
    "                                                                   smoothing_trend=smoothing_trend,\n",
    "                                                                   smoothing_seasonal=smoothing_seasonal)\n",
    "\n",
    "    if val is None:\n",
    "        # predictions for train and test\n",
    "        train_preds = EXPmodel.predict(start=0,end=len(train)-1)\n",
    "        test_preds = EXPmodel.predict(start=len(train), end=len(train)+len(test)-1)\n",
    "        return train_preds, test_preds\n",
    "    \n",
    "    else:\n",
    "        # predictions for train, val, test\n",
    "        train_preds = EXPmodel.predict(start=0,end=len(train)-1)\n",
    "        val_preds = EXPmodel.predict(start=len(train), end=len(train)+len(val)-1)\n",
    "        test_preds = EXPmodel.predict(start=len(train)+len(val), end=len(train)+len(val)+len(test)-1)\n",
    "        return train_preds, val_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a3d748",
   "metadata": {
    "papermill": {
     "duration": 0.021525,
     "end_time": "2023-08-23T10:17:51.576811",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.555286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### SARIMAX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa13c74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:51.623068Z",
     "iopub.status.busy": "2023-08-23T10:17:51.622270Z",
     "iopub.status.idle": "2023-08-23T10:17:51.633990Z",
     "shell.execute_reply": "2023-08-23T10:17:51.633150Z"
    },
    "papermill": {
     "duration": 0.037908,
     "end_time": "2023-08-23T10:17:51.636432",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.598524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sarimax_model(X_train, X_test, y_train, y_test, X_val=None, y_val=None, **model_parameters):\n",
    "    '''\n",
    "    Creates a SARIMAX model and returns predictions for the train and test data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pandas.core.frame.DataFrame\n",
    "              Train dataset that will be used for training the model (features)\n",
    "    y_train : pandas.core.frame.DataFrame\n",
    "              Train dataset that will be used for training the model (target variable)\n",
    "    X_test : pandas.core.frame.DataFrame\n",
    "             Test dataset that will be used for evaluating the model (features)\n",
    "    y_test : pandas.core.frame.DataFrame\n",
    "             Test dataset that will be used for evaluating the model (target variable)\n",
    "    X_val : NoneType or pandas.core.frame.DataFrame\n",
    "            Default : None\n",
    "            Validation dataset that will be used for evaluating the model (features)\n",
    "    y_val : NoneType or pandas.core.frame.DataFrame\n",
    "            Default : None\n",
    "            Validation dataset that will be used for evaluating the model (target variable)\n",
    "    model_parameters : kwargs\n",
    "                       Keyword argument that can be used to provide parameters to the model\n",
    "                       Can take statsmodels.tsa.statespace.sarimax.SARIMAX as parameters\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    train_preds : pandas.core.series.Series\n",
    "                  Predictions made by the model on training dataset\n",
    "    val_preds : pandas.core.series.Series\n",
    "                Predictions made by the model on validation dataset (If X_val, y_val is not None)\n",
    "    test_preds : pandas.core.series.Series\n",
    "                 Predictions made by the model on testing dataset\n",
    "    '''\n",
    "    sarimax_model_fit = SARIMAX(y_train, exog=X_train, **model_parameters).fit()\n",
    "    \n",
    "    if X_val is None or y_val is None:\n",
    "        train_preds = sarimax_model_fit.predict(start=1,end=len(X_train)-1, exog=X_train)\n",
    "        test_preds = sarimax_model_fit.predict(start=len(X_train), end=len(X_train)+len(X_test)-1, exog=X_test)\n",
    "        return train_preds, test_preds\n",
    "    \n",
    "    else:\n",
    "        train_preds = sarimax_model_fit.predict(start=1,end=len(y_train)-1, exog=X_train)\n",
    "        val_preds = sarimax_model_fit.predict(start=len(y_train), end=len(y_train)+len(y_val)-1, exog=X_val)\n",
    "        test_preds = sarimax_model_fit.predict(start=len(y_train)+len(y_val), end=len(y_train)+len(y_val)+len(y_test)-1, exog=pd.concat([X_val,X_test]))\n",
    "        test_preds = test_preds[-18:]\n",
    "        return train_preds, val_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92d05f",
   "metadata": {
    "papermill": {
     "duration": 0.021588,
     "end_time": "2023-08-23T10:17:51.680083",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.658495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52113449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:51.726004Z",
     "iopub.status.busy": "2023-08-23T10:17:51.725171Z",
     "iopub.status.idle": "2023-08-23T10:17:51.731363Z",
     "shell.execute_reply": "2023-08-23T10:17:51.730560Z"
    },
    "papermill": {
     "duration": 0.031749,
     "end_time": "2023-08-23T10:17:51.733492",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.701743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluation_metrics(actual, preds):\n",
    "    '''\n",
    "    Calculates MAE for the specified actual and predicted values\n",
    "    Length of actual and preds should be equal\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : Actual values\n",
    "    preds : Predicted values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    error_mae : float\n",
    "                Mean average error\n",
    "    '''\n",
    "    # if actual does not equal to preds then raise error\n",
    "    if len(actual)!=len(preds):\n",
    "        raise ValueError(\"Length of actual does not equal to length of preds\")\n",
    "    \n",
    "    error_mae = mae(actual, preds)\n",
    "    return error_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f1dae",
   "metadata": {
    "papermill": {
     "duration": 0.021439,
     "end_time": "2023-08-23T10:17:51.779175",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.757736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Prediction Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f475c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:51.824485Z",
     "iopub.status.busy": "2023-08-23T10:17:51.823770Z",
     "iopub.status.idle": "2023-08-23T10:17:51.830638Z",
     "shell.execute_reply": "2023-08-23T10:17:51.829597Z"
    },
    "papermill": {
     "duration": 0.032403,
     "end_time": "2023-08-23T10:17:51.833090",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.800687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction_plot(plot_title, plot_train, plot_test, plot_preds):\n",
    "    '''\n",
    "    Creates a matplotlib plot that displays train, test, and predicted values over time\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    plot_title : Title of the plot\n",
    "    plot_train : Actual data values over time\n",
    "    plot_test : Test values over time\n",
    "    plot_preds : Predicted values over time\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Matplotlib line chart\n",
    "    '''\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.title(plot_title)\n",
    "    plt.plot(plot_train, label='Train')\n",
    "    plt.plot(plot_test, label='Test')\n",
    "    plt.plot(plot_preds, label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a3b0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:51.879865Z",
     "iopub.status.busy": "2023-08-23T10:17:51.879247Z",
     "iopub.status.idle": "2023-08-23T10:17:51.885889Z",
     "shell.execute_reply": "2023-08-23T10:17:51.885141Z"
    },
    "papermill": {
     "duration": 0.032841,
     "end_time": "2023-08-23T10:17:51.888032",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.855191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def val_prediction_plot(plot_title, plot_train, plot_val, plot_test, plot_val_preds, plot_test_preds):\n",
    "    '''\n",
    "    Creates a matplotlib plot that displays train, test, and predicted values over time\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    plot_title : Title of the plot\n",
    "    plot_train : Actual data values over time\n",
    "    plot_val : Validation values over time\n",
    "    plot_test : Test values over time\n",
    "    plot_val_preds : Predicted values on validation data over time\n",
    "    plot_test_preds : Predicted_values on test data over time\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Matplotlib line chart\n",
    "    '''\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.title(plot_title)\n",
    "    plt.plot(plot_train, label='Train')\n",
    "    plt.plot(plot_val, label='Validation')\n",
    "    plt.plot(plot_test, label='Test')\n",
    "    plt.plot(plot_val_preds, label='Validation Prediction')\n",
    "    plt.plot(plot_test_preds, label='Test Predictions')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfacbd",
   "metadata": {
    "papermill": {
     "duration": 0.021254,
     "end_time": "2023-08-23T10:17:51.931251",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.909997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### All grid searches (SARIMA, ETS, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9de3947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:51.977267Z",
     "iopub.status.busy": "2023-08-23T10:17:51.976839Z",
     "iopub.status.idle": "2023-08-23T10:17:51.994627Z",
     "shell.execute_reply": "2023-08-23T10:17:51.993606Z"
    },
    "papermill": {
     "duration": 0.043555,
     "end_time": "2023-08-23T10:17:51.997108",
     "exception": false,
     "start_time": "2023-08-23T10:17:51.953553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_search_sarima(train, val,\n",
    "                       p_range=range(0,2), d_range=range(0,2), q_range=range(0,2),\n",
    "                       P_range=range(0,2), D_range=range(0,2), Q_range=range(0,2), m=[52]):\n",
    "    '''\n",
    "    Performs a grid search on statsmodels.tsa.statespace.sarimax.SARIMAX\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : Pandas.core.frame.DataFrame\n",
    "            Train set\n",
    "    val : Pandas.core.frame.DataFrame\n",
    "          Validation set\n",
    "    p_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of trend autoregression order\n",
    "    d_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of trend difference order\n",
    "    q_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of trend moving average order\n",
    "    P_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of seasonal autoregression order\n",
    "    D_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of seasonal difference order\n",
    "    Q_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of seasonal moving average order\n",
    "    m : list\n",
    "        Default : [52]\n",
    "        The number of time steps for a single seasonal period\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid : pandas.core.frame.DataFrame\n",
    "           Returns a dataframe with columns=['order', 'seasonal_order', 'train_error', 'val_error', 'test_error']\n",
    "    best_params : dict({'order':order_parameters, 'seasonal_order':seasonal_order_parameters})\n",
    "                  Returns a key-value pair of best performing model\n",
    "    '''\n",
    "    # check if train and test are pandas dataframe\n",
    "    if not isinstance(train, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"train is not a pandas dataframe\")\n",
    "  \n",
    "    if not isinstance(val, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"val is not a pandas dataframe\")\n",
    "        \n",
    "    # check if the range variables are python range\n",
    "    range_inputs = [p_range, d_range, q_range, P_range, D_range, Q_range]\n",
    "    for i in range_inputs:\n",
    "        if not isinstance(i, range):\n",
    "            raise TypeError(\"Range inputs are not of type range\")\n",
    "    \n",
    "    # check if m is a python list\n",
    "    if not isinstance(m, list):\n",
    "        raise TypeError(\"m is not a list\")\n",
    "\n",
    "    params_combinations = list(itertools.product(p_range, d_range, q_range, P_range, D_range, Q_range, m))\n",
    "    \n",
    "    # defining return variables\n",
    "    best_error=float(\"inf\")\n",
    "    best_params=None\n",
    "    \n",
    "    # defining output of each value as pandas dataframe\n",
    "    grid = pd.DataFrame(columns=['order', 'seasonal_order', 'train_error', 'val_error'])\n",
    "    \n",
    "    # unpacking itertools product params_combinations\n",
    "    for p, d, q, P, D, Q, m in params_combinations:\n",
    "        order = (p, d, q)\n",
    "        seasonal_order = (P, D, Q, m)\n",
    "        \n",
    "        # try except for handling exceptions\n",
    "        try:\n",
    "            # create the model and returns predictions for train and test\n",
    "            train_preds, val_preds = sarima_model(train, val, order=order, seasonal_order=seasonal_order)\n",
    "\n",
    "            # if preds are nan then handle\n",
    "            if(np.isnan(np.min(train_preds)) or np.isnan(np.min(val_preds))):\n",
    "                grid.loc[len(grid)] = [order, seasonal_order, 'nan', 'nan']\n",
    "            \n",
    "            # if preds are non-nan\n",
    "            # evaluates the model and assigns best error and best params if the test/val error is least\n",
    "            else:\n",
    "                train_error = evaluation_metrics(train[1:], train_preds)\n",
    "                val_error = evaluation_metrics(val, val_preds)\n",
    "\n",
    "                grid.loc[len(grid)] = [order, seasonal_order, train_error, val_error]\n",
    "\n",
    "            if(val_error<best_error):\n",
    "                best_error=val_error\n",
    "                best_params = {'order':order, 'seasonal_order':seasonal_order}\n",
    "    \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # returning best parameters\n",
    "    return grid, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43eb5801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:52.044419Z",
     "iopub.status.busy": "2023-08-23T10:17:52.044021Z",
     "iopub.status.idle": "2023-08-23T10:17:52.063795Z",
     "shell.execute_reply": "2023-08-23T10:17:52.062633Z"
    },
    "papermill": {
     "duration": 0.046566,
     "end_time": "2023-08-23T10:17:52.066313",
     "exception": false,
     "start_time": "2023-08-23T10:17:52.019747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_search_ets(train, val,\n",
    "                    trend=['add', 'mul'], seasonal=['add','mul'], seasonal_periods=[52], \n",
    "                    smoothing_values=np.arange(0.0,0.3,0.1)):\n",
    "    '''\n",
    "    Performs a grid search on statsmodels.tsa.holtwinters.ExponentialSmoothing\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : Pandas.core.frame.DataFrame\n",
    "            Train set\n",
    "    val : Pandas.core.frame.DataFrame\n",
    "          Validation set\n",
    "    trend : list\n",
    "            Default : ['add', 'mul']\n",
    "            Trend component of statsmodels.tsa.holtwinters.ExponentialSmoothing\n",
    "            Can have only 'add' and/or 'mul' as values inside list\n",
    "    seasonal : list\n",
    "               Default : ['add', 'mul']\n",
    "               Seasonal component of statsmodels.tsa.holtwinters.ExponentialSmoothing\n",
    "               Can have only 'add' and/or 'mul' as values inside list\n",
    "    seasonal_periods : list\n",
    "                       Default : [52]\n",
    "                       Seasonal_periods component of statsmodels.tsa.holtwinters.ExponentialSmoothing\n",
    "    smoothing_values : numpy.ndarray\n",
    "                       Default : np.arange(0.0,0.3,0.1)\n",
    "                       Set of values that can be used for for smoothing_level, smoothing_trend, smoothing_seasonal values of\n",
    "                       statsmodels.tsa.holtwinters.ExponentialSmoothing.fit\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid : pandas.core.frame.DataFrame\n",
    "           Returns a dataframe with columns=['trend', 'seasonal', 'seasonal_periods', \n",
    "                                             'smoothing_level', 'smoothing_trend', 'smoothing_seasonal',\n",
    "                                             'train_error', 'val_error', 'test_error']\n",
    "    best_params : dict({'trend':trend,'seasonal':seasonal,'seasonal_periods':seasonal_periods,\n",
    "                        'smoothing_level':smoothing_level, 'smoothing_trend':smoothing_trend,'smoothing_seasonal':smoothing_seasonal})\n",
    "                  Returns a key-value pair of the best performing model\n",
    "              \n",
    "    '''\n",
    "    # check if train and test are pandas dataframe\n",
    "    if not isinstance(train, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"train is not a pandas dataframe\")\n",
    "     \n",
    "    if not isinstance(val, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"val is not a pandas dataframe\")\n",
    "    \n",
    "    \n",
    "    list_inputs = [trend, seasonal]    \n",
    "    for col in list_inputs:\n",
    "        # check if the list variables are python range\n",
    "        if not isinstance(col, list):\n",
    "            raise TypeError(\"Trend and/or seasonal is not of type list\")\n",
    "            \n",
    "        # check len of list_inputs as it can only have 1 or 2 values\n",
    "        if(len(col)>2 or len(col)<1):\n",
    "            raise ValueError(\"Trend and seasonal can only have 1 or 2 values\")\n",
    "\n",
    "        for i in col:\n",
    "            if(i!='add' and i!='mul'):\n",
    "                raise ValueError(\"Trend and seasonal can only have 'add' and/or 'mul' as values in the list\")\n",
    "        \n",
    "    # check if smoothing_values is of correct type\n",
    "    if not isinstance(smoothing_values, np.ndarray):\n",
    "        raise TypeError(\"Smoothing range is not of type \")\n",
    "\n",
    "    smoothing_level = smoothing_values\n",
    "    smoothing_trend = smoothing_values\n",
    "    smoothing_seasonal = smoothing_values\n",
    "    \n",
    "    params_combinations = list(itertools.product(trend, seasonal, seasonal_periods, \n",
    "                                                 smoothing_level, smoothing_trend, smoothing_seasonal))\n",
    "    \n",
    "    # defining return variables\n",
    "    best_error=float(\"inf\")\n",
    "    best_params=None\n",
    "    \n",
    "    # defining output of each value as pandas dataframe\n",
    "    grid = pd.DataFrame(columns=['trend', 'seasonal', 'seasonal_periods', \n",
    "                                 'smoothing_level', 'smoothing_trend', 'smoothing_seasonal',\n",
    "                                 'train_error', 'val_error'])\n",
    "    \n",
    "    # unpacking itertools product params_combinations\n",
    "    for trend, seasonal, seasonal_periods, alpha, beta, gamma in params_combinations:\n",
    "        \n",
    "        # try except for handling exceptions\n",
    "        try:\n",
    "            # create the model and returns predictions for train and test\n",
    "            train_preds, val_preds = ets_model(train, val,\n",
    "                                               trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods,\n",
    "                                               smoothing_level=alpha, smoothing_trend=beta, smoothing_seasonal=gamma)\n",
    "\n",
    "            # if preds are nan then handle\n",
    "            if(np.isnan(np.min(train_preds)) or np.isnan(np.min(val_preds))):\n",
    "                grid.loc[len(grid)] = [trend, seasonal, seasonal_periods,\n",
    "                                       alpha, beta, gamma, \n",
    "                                       'nan', 'nan']\n",
    "            \n",
    "            # if preds are non-nan\n",
    "            # evaluates the model and assigns best error and best params if the test/val error is least\n",
    "            else:\n",
    "                train_error = evaluation_metrics(train, train_preds)\n",
    "                val_error = evaluation_metrics(val, val_preds)\n",
    "\n",
    "                grid.loc[len(grid)] = [trend, seasonal, seasonal_periods,\n",
    "                                       alpha, beta, gamma,\n",
    "                                       train_error, val_error]\n",
    "\n",
    "            if(val_error<best_error):\n",
    "                best_error=val_error\n",
    "                best_params = {'trend':trend,\n",
    "                               'seasonal':seasonal,\n",
    "                               'seasonal_periods':seasonal_periods,\n",
    "                               'smoothing_level':alpha,\n",
    "                               'smoothing_trend':beta,\n",
    "                               'smoothing_seasonal':gamma}\n",
    "    \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # printing the grid dataframe and returning best parameters\n",
    "    return grid, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60a19374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:52.113753Z",
     "iopub.status.busy": "2023-08-23T10:17:52.113363Z",
     "iopub.status.idle": "2023-08-23T10:17:52.129876Z",
     "shell.execute_reply": "2023-08-23T10:17:52.128812Z"
    },
    "papermill": {
     "duration": 0.043157,
     "end_time": "2023-08-23T10:17:52.132327",
     "exception": false,
     "start_time": "2023-08-23T10:17:52.089170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_search_sarimax(X_train, X_val, y_train, y_val,\n",
    "                        p_range=range(0,2), d_range=range(0,2), q_range=range(0,2),\n",
    "                        P_range=range(0,2), D_range=range(0,2), Q_range=range(0,2), m=[52]):\n",
    "    '''\n",
    "    Performs a grid search on statsmodels.tsa.statespace.sarimax.SARIMAX\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : Pandas.core.frame.DataFrame\n",
    "              Train set exogenous variables\n",
    "    X_val : Pandas.core.frame.DataFrame\n",
    "            Validation set exogenous variables\n",
    "    y_train : Pandas.core.frame.DataFrame\n",
    "              Train set output variable\n",
    "    y_val : Pandas.core.frame.DataFrame\n",
    "            Validation set output variable\n",
    "    p_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of trend autoregression order\n",
    "    d_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of trend difference order\n",
    "    q_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of trend moving average order\n",
    "    P_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of seasonal autoregression order\n",
    "    D_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of seasonal difference order\n",
    "    Q_range : range\n",
    "              Default : range(0,2)\n",
    "              Range of seasonal moving average order\n",
    "    m : list\n",
    "        Default : [52]\n",
    "        The number of time steps for a single seasonal period\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid : pandas.core.frame.DataFrame\n",
    "           Returns a dataframe with columns=['order', 'seasonal_order', 'train_error', 'val_error', 'test_error']\n",
    "    best_params : dict({'order':order_parameters, 'seasonal_order':seasonal_order_parameters})\n",
    "                  Returns a key-value pair of best performing model\n",
    "    '''\n",
    "        \n",
    "    # check if the range variables are python range\n",
    "    range_inputs = [p_range, d_range, q_range, P_range, D_range, Q_range]\n",
    "    for i in range_inputs:\n",
    "        if not isinstance(i, range):\n",
    "            raise TypeError(\"Range inputs are not of type range\")\n",
    "    \n",
    "    # check if m is a python list\n",
    "    if not isinstance(m, list):\n",
    "        raise TypeError(\"m is not a list\")\n",
    "\n",
    "    params_combinations = list(itertools.product(p_range, d_range, q_range, P_range, D_range, Q_range, m))\n",
    "    \n",
    "    # defining return variables\n",
    "    best_error=float(\"inf\")\n",
    "    best_params=None\n",
    "    \n",
    "    # defining output of each value as pandas dataframe\n",
    "    grid = pd.DataFrame(columns=['order', 'seasonal_order', 'train_error', 'val_error'])\n",
    "    \n",
    "    # unpacking itertools product params_combinations\n",
    "    for p, d, q, P, D, Q, m in params_combinations:\n",
    "        order = (p, d, q)\n",
    "        seasonal_order = (P, D, Q, m)\n",
    "        \n",
    "        # try except for handling exceptions\n",
    "        try:\n",
    "            # create the model and returns predictions for train and test\n",
    "            train_preds, val_preds = sarimax_model(X_train, X_val, y_train, y_val, order=order, seasonal_order=seasonal_order)\n",
    "\n",
    "            # if preds are nan then handle\n",
    "            if(np.isnan(np.min(train_preds)) or np.isnan(np.min(val_preds))):\n",
    "                grid.loc[len(grid)] = [order, seasonal_order, 'nan', 'nan']\n",
    "            \n",
    "            # if preds are non-nan\n",
    "            # evaluates the model and assigns best error and best params if the test/val error is least\n",
    "            else:\n",
    "                train_error = evaluation_metrics(y_train[1:], train_preds)\n",
    "                val_error = evaluation_metrics(y_val, val_preds)\n",
    "\n",
    "                grid.loc[len(grid)] = [order, seasonal_order, train_error, val_error]\n",
    "\n",
    "            if(val_error<best_error):\n",
    "                best_error=val_error\n",
    "                best_params = {'order':order, 'seasonal_order':seasonal_order}\n",
    "    \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # returning best parameters\n",
    "    return grid, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3a515f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:52.179445Z",
     "iopub.status.busy": "2023-08-23T10:17:52.178672Z",
     "iopub.status.idle": "2023-08-23T10:17:52.187896Z",
     "shell.execute_reply": "2023-08-23T10:17:52.187113Z"
    },
    "papermill": {
     "duration": 0.035375,
     "end_time": "2023-08-23T10:17:52.190043",
     "exception": false,
     "start_time": "2023-08-23T10:17:52.154668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_search(train, val, model_name, **parameters):\n",
    "    '''\n",
    "    Performs a grid search on the given model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : Pandas.core.frame.DataFrame\n",
    "            Train set\n",
    "    val : Pandas.core.frame.DataFrame\n",
    "          Validation set\n",
    "    test : Pandas.core.frame.DataFrame\n",
    "           Test set\n",
    "    model_name : {'SARIMA', 'ETS'} (Can accept this in upper/lower/mixed case)\n",
    "                 SARIMA : Creates a SARIMA model\n",
    "                 ETS : Creates a triple Exponential Smoothing model\n",
    "    **parameters : kwargs\n",
    "                   Parameters that can be passed in grid search\n",
    "                   \n",
    "    Returns\n",
    "    -------\n",
    "    grid : pandas.core.frame.DataFrame\n",
    "           Returns a dataframe of all the results of exhaustive search\n",
    "           \n",
    "           If model_name = 'SARIMA' : Returns a dataframe with \n",
    "                                      columns=['order', 'seasonal_order', 'train_error', 'test_error']\n",
    "           If model_name = 'ETS' : Returns a dataframe with \n",
    "                                   columns=['trend', 'seasonal', 'seasonal_periods',\n",
    "                                            'smoothing_level', 'smoothing_trend', 'smoothing_seasonal',\n",
    "                                            'train_error', 'test_error']\n",
    "                                                         \n",
    "    best_params : Dictionary\n",
    "                  Returns a key-value pair of the best performing model   \n",
    "                  If model_name = 'SARIMA' : dict({'order':order_parameters, \n",
    "                                                   'seasonal_order':seasonal_order_parameters})\n",
    "                  If model_name = 'ETS' : dict({'trend':trend,\n",
    "                                                'seasonal':seasonal,\n",
    "                                                'seasonal_periods':seasonal_periods,\n",
    "                                                'smoothing_level':smoothing_level, \n",
    "                                                'smoothing_trend':smoothing_trend,\n",
    "                                                'smoothing_seasonal':smoothing_seasonal})\n",
    "    '''\n",
    "    # if model_name is not sarima or ets (uppercase/lowercase both works for user)\n",
    "    model_name = model_name.upper()\n",
    "    if (model_name not in ['SARIMA', 'ETS']):\n",
    "        raise ValueError(\"model_name must be one of {'SARIMA', 'ETS'}\")\n",
    "    \n",
    "    model_name = model_name.upper()\n",
    "    # Performs SARIMA grid search\n",
    "    if(model_name=='SARIMA'):\n",
    "        grid, best_parameters = grid_search_sarima(train, val, **parameters)\n",
    "\n",
    "    # Performs ETS grid search\n",
    "    elif(model_name=='ETS'):\n",
    "        grid, best_parameters = grid_search_ets(train, val, **parameters)\n",
    "    \n",
    "    return grid, best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a75470",
   "metadata": {
    "papermill": {
     "duration": 0.022111,
     "end_time": "2023-08-23T10:17:52.234457",
     "exception": false,
     "start_time": "2023-08-23T10:17:52.212346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Generate Predictions Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7a49df8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:52.282276Z",
     "iopub.status.busy": "2023-08-23T10:17:52.281300Z",
     "iopub.status.idle": "2023-08-23T10:17:52.312158Z",
     "shell.execute_reply": "2023-08-23T10:17:52.311198Z"
    },
    "papermill": {
     "duration": 0.057876,
     "end_time": "2023-08-23T10:17:52.314900",
     "exception": false,
     "start_time": "2023-08-23T10:17:52.257024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_predictions_full(dataset, model_name, splits=0.8, perform_grid_search=False, **kwargs):\n",
    "    '''\n",
    "    Returns predictions for the specified model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pandas.core.frame.DataFrame\n",
    "              Pandas univariate dataframe to be used for predictions\n",
    "    \n",
    "    model_name : {'SARIMA', 'ETS'} (Can accept in upper/lower/mixed case)\n",
    "                 SARIMA : Creates a SARIMA model by calling the sarima_model function\n",
    "                 ETS : Creates a triple Exponential Smoothing model by calling the ets_model function\n",
    "    splits : float, tuple\n",
    "             Default = 0.8; splitting data in two sets - train and test of size 0.8 and 0.2 respectively\n",
    "             float : Splits in two sets - train and test\n",
    "                     Only accepts value between range (0.0, 1.0)\n",
    "             tuple(train_size, validation_size) : Splits in three sets - train, validation and test\n",
    "                                                  train_size -> Train size\n",
    "                                                  validation_size -> Validation size\n",
    "                                                  Should only contain values between range (0.0, 1.0) and \n",
    "                                                  \"train_size + test_size\" should also be between (0.0, 1.0)\n",
    "             Splits should be tuple if perform_grid_search is True\n",
    "    perform_grid_search : boolean\n",
    "                          Default = False; does not perform grid search\n",
    "    **kwargs : kwargs\n",
    "               Keyword arguments that can be used to provide parameters to the model or grid search\n",
    "               If perform_grid_search=False & model_name='SARIMA' : kwargs =  statsmodels.tsa.statespace.sarimax.SARIMAX parameters\n",
    "               If perform_grid_search=False & model_name='ETS' : kwargs =  statsmodels.tsa.holtwinters.ExponentialSmoothing parameters\n",
    "               If perform_grid_search=True & model_name='SARIMA' : kwargs = grid_search_sarima function parameters\n",
    "               If perform_grid_search=True & model_name='ETS' : kwargs = grid_search_ets function parameters\n",
    "                                                      \n",
    "    Prints\n",
    "    ------\n",
    "    Split size : float\n",
    "                 If splits is float - Train and test splits \n",
    "                 If splits is a tuple - Train, validation, and test splits\n",
    "    Grid Search Parameters and Range : Parameters provided by the user for grid search(if perform_grid_search==True)\n",
    "    Best Parameters : Best parameters found by the grid search(if perform_grid_search==True)\n",
    "    Model Parameters : Model parameters provided by the user (if perform_grid_search==False)\n",
    "    Evaluation : Prints a dataframe containing \"MAE\" for train, val, test\n",
    "    Prediction plot : Predictions on val and test data as a plot\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    val_preds : Predictions for val split (if splits is tuple)\n",
    "    test_preds : Predictions for test split\n",
    "    '''\n",
    "    # check dataset type\n",
    "    if not isinstance(dataset, pd.core.frame.DataFrame):\n",
    "        raise TypeError(\"train is not a pandas dataframe\")\n",
    "        \n",
    "    # check splits type\n",
    "    if not isinstance(splits, (float, tuple)):\n",
    "        raise TypeError(\"splits is not float or tuple\")\n",
    "    \n",
    "    # prepares data and returns train, test if splits are float\n",
    "    if isinstance(splits, float):\n",
    "        train, test = prepare_data(dataset, splits)\n",
    "    # prepares data and returns train, val, test if splits are tuple\n",
    "    elif isinstance(splits, tuple):\n",
    "        train, val, test = prepare_data(dataset, splits)\n",
    "    \n",
    "    # if model_name is not sarima or ets (uppercase/lowercase both works for user)\n",
    "    model_name = model_name.upper()\n",
    "    if (model_name not in ['SARIMA', 'ETS']):\n",
    "        raise ValueError(\"model_name must be one of {'SARIMA', 'ETS'}\")\n",
    "    \n",
    "    # perform_grid_search should be bool\n",
    "    if not isinstance(perform_grid_search, bool):\n",
    "        raise TypeError(\"perform_grid_search should be boolean\")\n",
    "        \n",
    "    # if grid search then splits must be tuple for train/val/test\n",
    "    if(perform_grid_search==True):\n",
    "        if not isinstance(splits, tuple):\n",
    "            raise ValueError(\"If perform_grid_search is True then splits should be a tuple\")\n",
    "            \n",
    "    # print parameters\n",
    "    print(\"------------------Parameters------------------\")\n",
    "    # printing train/test split size\n",
    "    if isinstance(splits, float):\n",
    "        print(\"Train Split Size :\", splits)\n",
    "        print(\"Test Split Size :\", round(1.0-splits, 3))\n",
    "\n",
    "    # printing train/validation/test split size\n",
    "    if isinstance(splits, tuple):\n",
    "        print(\"Train Split Size :\", splits[0])\n",
    "        print(\"Validation Split Size :\", splits[1])\n",
    "        print(\"Test Split Size :\", round(1.0-splits[1]-splits[0],3))\n",
    "    \n",
    "    # If grid search then performs grid search and returns the grid dataframe(as grid) and the best model parameters(as kwargs)\n",
    "    if perform_grid_search==True:\n",
    "        # printing user input range\n",
    "        print(\"-----Parameters and Range for grid search-----\")\n",
    "        for key, value in kwargs.items():\n",
    "            print(key, \":\", value)\n",
    "            \n",
    "        grid, kwargs = grid_search(train, val, model_name, **kwargs)\n",
    "        print(\"-------------Best Parameters Found-------------\")\n",
    "        for key, value in kwargs.items():\n",
    "            print(key, \":\", value)\n",
    "        \n",
    "        if model_name=='SARIMA':\n",
    "            train_preds, val_preds, test_preds = sarima_model(train, test, val=val, **kwargs)\n",
    "            \n",
    "            train_error_mae = evaluation_metrics(train[1:], train_preds)\n",
    "            val_error_mae = evaluation_metrics(val, val_preds)\n",
    "            test_error_mae = evaluation_metrics(test, test_preds)\n",
    "        \n",
    "        elif model_name=='ETS':\n",
    "            train_preds, val_preds, test_preds = ets_model(train, test, val=val, **kwargs)\n",
    "\n",
    "            train_error_mae = evaluation_metrics(train, train_preds)\n",
    "            val_error_mae = evaluation_metrics(val, val_preds)\n",
    "            test_error_mae = evaluation_metrics(test, test_preds)\n",
    "    \n",
    "        # mae\n",
    "        error_df = pd.DataFrame([[round(train_error_mae, 4)],\n",
    "                                 [round(val_error_mae, 4)],\n",
    "                                 [round(test_error_mae, 4)]],\n",
    "                                columns = ['MAE'],\n",
    "                                index = ['Train', 'Val', 'Test'])\n",
    "    \n",
    "        val_prediction_plot(model_name, train, val, test, val_preds, test_preds)\n",
    "        print(\"------------------Evaluation------------------\")\n",
    "        print(error_df)\n",
    "    \n",
    "        return val_preds, test_preds\n",
    "    \n",
    "    # If no grid search\n",
    "    else:\n",
    "        print(\"------------------Parameters------------------\")\n",
    "        for key, value in kwargs.items():\n",
    "            print(key, \":\", value)\n",
    "   \n",
    "        if isinstance(splits, float):\n",
    "            # for sarima model perform\n",
    "            if model_name=='SARIMA':\n",
    "                train_preds, test_preds = sarima_model(train, test, **kwargs)\n",
    "\n",
    "                train_error_mae = evaluation_metrics(train[1:], train_preds)\n",
    "                test_error_mae = evaluation_metrics(test, test_preds)\n",
    "\n",
    "            # for ets perform\n",
    "            elif model_name=='ETS':\n",
    "                train_preds, test_preds = ets_model(train, test, **kwargs)\n",
    "\n",
    "                train_error_mae = evaluation_metrics(train, train_preds)\n",
    "                test_error_mae = evaluation_metrics(test, test_preds)\n",
    "\n",
    "            # mae\n",
    "            error_df = pd.DataFrame([[round(train_error_mae, 4)],\n",
    "                                     [round(test_error_mae, 4)]], \n",
    "                                    columns = ['MAE'], \n",
    "                                    index = ['Train', 'Test'])\n",
    "            \n",
    "            prediction_plot(model_name, train, test, test_preds)\n",
    "            print(\"------------------Evaluation------------------\")\n",
    "            print(error_df)\n",
    "\n",
    "            return test_preds\n",
    "        else:\n",
    "            if model_name=='SARIMA':\n",
    "                train_preds, val_preds, test_preds = sarima_model(train, test, val=val, **kwargs)\n",
    "                train_error_mae = evaluation_metrics(train[1:], train_preds)\n",
    "                val_error_mae = evaluation_metrics(val, val_preds)\n",
    "                test_error_mae = evaluation_metrics(test, test_preds)\n",
    "        \n",
    "            elif model_name=='ETS':\n",
    "                train_preds, val_preds, test_preds = ets_model(train, test, val=val, **kwargs)\n",
    "\n",
    "                train_error_mae = evaluation_metrics(train, train_preds)\n",
    "                val_error_mae = evaluation_metrics(val, val_preds)\n",
    "                test_error_mae = evaluation_metrics(test, test_preds)\n",
    "\n",
    "            # mae\n",
    "            error_df = pd.DataFrame([[round(train_error_mae, 4)],\n",
    "                                     [round(val_error_mae, 4)],\n",
    "                                     [round(test_error_mae, 4)]],\n",
    "                                    columns = ['MAE'],\n",
    "                                    index = ['Train', 'Val', 'Test'])\n",
    "            \n",
    "            val_prediction_plot(model_name, train, val, test, val_preds, test_preds)\n",
    "            print(\"------------------Evaluation------------------\")\n",
    "            print(error_df)\n",
    "\n",
    "            return val_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08aa388b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:52.362694Z",
     "iopub.status.busy": "2023-08-23T10:17:52.362235Z",
     "iopub.status.idle": "2023-08-23T10:17:52.389820Z",
     "shell.execute_reply": "2023-08-23T10:17:52.388534Z"
    },
    "papermill": {
     "duration": 0.054826,
     "end_time": "2023-08-23T10:17:52.392335",
     "exception": false,
     "start_time": "2023-08-23T10:17:52.337509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sarimax_full(dataset, splits=0.8, perform_grid_search=False, **kwargs):\n",
    "    '''\n",
    "    Returns predictions for the specified model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pandas.core.frame.DataFrame\n",
    "              Pandas univariate dataframe to be used for predictions\n",
    "    splits : float, tuple\n",
    "             Default = 0.8; splitting data in two sets - train and test of size 0.8 and 0.2 respectively\n",
    "             float : Splits in two sets - train and test\n",
    "                     Only accepts value between range (0.0, 1.0)\n",
    "             tuple(train_size, validation_size) : Splits in three sets - train, validation and test\n",
    "                                                  train_size -> Train size\n",
    "                                                  validation_size -> Validation size\n",
    "                                                  Should only contain values between range (0.0, 1.0) and \n",
    "                                                  \"train_size + test_size\" should also be between (0.0, 1.0)\n",
    "             Splits should be tuple if perform_grid_search is True\n",
    "    perform_grid_search : boolean\n",
    "                          Default = False; does not perform grid search\n",
    "    **kwargs : kwargs\n",
    "               Keyword arguments that can be used to provide parameters to the model or grid search\n",
    "               If perform_grid_search=False : kwargs =  statsmodels.tsa.statespace.sarimax.SARIMAX parameters\n",
    "               If perform_grid_search=True : kwargs = grid_search_sarimax function parameters\n",
    "                                                      \n",
    "    Prints\n",
    "    ------\n",
    "    Split size : float\n",
    "                 If splits is float - Train and test splits \n",
    "                 If splits is a tuple - Train, validation, and test splits\n",
    "    Grid Search Parameters and Range : Parameters provided by the user for grid search(if perform_grid_search==True)\n",
    "    Best Parameters : Best parameters found by the grid search(if perform_grid_search==True)\n",
    "    Model Parameters : Model parameters provided by the user (if perform_grid_search==False)\n",
    "    Evaluation : Prints a dataframe containing \"MAE\" for train, val, test\n",
    "    Prediction plot : Predictions on val and test data as a plot\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    val_preds : Predictions for val split (if splits is tuple)\n",
    "    test_preds : Predictions for test split\n",
    "    '''\n",
    "    # perform_grid_search should be bool\n",
    "    if not isinstance(perform_grid_search, bool):\n",
    "        raise TypeError(\"perform_grid_search should be boolean\")\n",
    "        \n",
    "    # if grid search then splits must be tuple for train/val/test\n",
    "    if(perform_grid_search==True):\n",
    "        if not isinstance(splits, tuple):\n",
    "            raise ValueError(\"If perform_grid_search is True then splits should be a tuple\")\n",
    "            \n",
    "    # print parameters\n",
    "    print(\"------------------Parameters------------------\")\n",
    "    # printing train/test split size\n",
    "    if isinstance(splits, float):\n",
    "        print(\"Train Split Size :\", splits)\n",
    "        print(\"Test Split Size :\", round(1.0-splits, 3))\n",
    "\n",
    "    # printing train/validation/test split size\n",
    "    if isinstance(splits, tuple):\n",
    "        print(\"Train Split Size :\", splits[0])\n",
    "        print(\"Validation Split Size :\", splits[1])\n",
    "        print(\"Test Split Size :\", round(1.0-splits[1]-splits[0],3))\n",
    "            \n",
    "    # prepares data and returns train, test if splits are float\n",
    "    if isinstance(splits, float):\n",
    "        train, test = prepare_data(dataset, splits)\n",
    "        train_Y = train['Weekly_Sales']\n",
    "        train_X = train.drop(columns='Weekly_Sales')\n",
    "\n",
    "        test_Y = test['Weekly_Sales']\n",
    "        test_X = test.drop(columns='Weekly_Sales')\n",
    "    # prepares data and returns train, val, test if splits are tuple\n",
    "    elif isinstance(splits, tuple):\n",
    "        train, val, test = prepare_data(dataset, splits)\n",
    "        train_Y = train['Weekly_Sales']\n",
    "        train_X = train.drop(columns='Weekly_Sales')\n",
    "        \n",
    "        val_Y = val['Weekly_Sales']\n",
    "        val_X = val.drop(columns='Weekly_Sales')\n",
    "        \n",
    "        test_Y = test['Weekly_Sales']\n",
    "        test_X = test.drop(columns='Weekly_Sales')\n",
    "    \n",
    "    if perform_grid_search==True:\n",
    "        # printing user input range\n",
    "        print(\"-----Parameters and Range for grid search-----\")\n",
    "        for key, value in kwargs.items():\n",
    "            print(key, \":\", value)\n",
    "\n",
    "        grid, kwargs = grid_search_sarimax(train_X, val_X, train_Y, val_Y, **kwargs)\n",
    "        print(\"-------------Best Parameters Found-------------\")\n",
    "        for key, value in kwargs.items():\n",
    "            print(key, \":\", value)\n",
    "            \n",
    "        train_preds, val_preds, test_preds = sarimax_model(train_X, test_X, train_Y, test_Y, X_val=val_X, y_val=val_Y, **kwargs)\n",
    "        train_error_mae = evaluation_metrics(train_Y[1:], train_preds)\n",
    "        val_error_mae = evaluation_metrics(val_Y, val_preds)\n",
    "        test_error_mae = evaluation_metrics(test_Y, test_preds)\n",
    "    \n",
    "        # mae\n",
    "        error_df = pd.DataFrame([[round(train_error_mae, 4)],\n",
    "                                 [round(val_error_mae, 4)],\n",
    "                                 [round(test_error_mae, 4)]],\n",
    "                                columns = ['MAE'],\n",
    "                                index = ['Train', 'Val', 'Test'])\n",
    "    \n",
    "        val_prediction_plot(\"SARIMAX\", train_Y, val_Y, test_Y, val_preds, test_preds)\n",
    "        print(\"------------------Evaluation------------------\")\n",
    "        print(error_df)\n",
    "    \n",
    "        return val_preds, test_preds\n",
    "    \n",
    "    else:\n",
    "        print(\"------------------Parameters------------------\")\n",
    "        for key, value in kwargs.items():\n",
    "            print(key, \":\", value)\n",
    "        \n",
    "        if isinstance(splits, float):         \n",
    "            train_preds, test_preds = sarimax_model(train_X, test_X, train_Y, test_Y, **kwargs)\n",
    "            \n",
    "            train_error_mae = evaluation_metrics(train_Y[1:], train_preds)\n",
    "            test_error_mae = evaluation_metrics(test_Y, test_preds)\n",
    "            # mae\n",
    "            error_df = pd.DataFrame([[round(train_error_mae, 4)],\n",
    "                                     [round(test_error_mae, 4)]], \n",
    "                                    columns = ['MAE'], \n",
    "                                    index = ['Train', 'Test'])\n",
    "            \n",
    "            prediction_plot(\"SARIMAX\", train_Y, test_Y, test_preds)\n",
    "            print(\"------------------Evaluation------------------\")\n",
    "            print(error_df)\n",
    "\n",
    "            return test_preds\n",
    "        \n",
    "        else:\n",
    "            train_preds, val_preds, test_preds = sarimax_model(train_X, test_X, train_Y, test_Y, val_X, val_Y, **kwargs)\n",
    "            train_error_mae = evaluation_metrics(train_Y[1:], train_preds)\n",
    "            val_error_mae = evaluation_metrics(val_Y, val_preds)\n",
    "            test_error_mae = evaluation_metrics(test_Y, test_preds)\n",
    "\n",
    "            # mae\n",
    "            error_df = pd.DataFrame([[round(train_error_mae, 4)],\n",
    "                                     [round(val_error_mae, 4)],\n",
    "                                     [round(test_error_mae, 4)]],\n",
    "                                    columns = ['MAE'],\n",
    "                                    index = ['Train', 'Val', 'Test'])\n",
    "            \n",
    "            val_prediction_plot(\"SARIMAX\", train_Y, val_Y, test_Y, val_preds, test_preds)\n",
    "            print(\"------------------Evaluation------------------\")\n",
    "            print(error_df)\n",
    "\n",
    "            return val_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13d08b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:52.442801Z",
     "iopub.status.busy": "2023-08-23T10:17:52.440756Z",
     "iopub.status.idle": "2023-08-23T10:17:52.452859Z",
     "shell.execute_reply": "2023-08-23T10:17:52.452030Z"
    },
    "papermill": {
     "duration": 0.039495,
     "end_time": "2023-08-23T10:17:52.455543",
     "exception": false,
     "start_time": "2023-08-23T10:17:52.416048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_predictions(dataset, model_name, splits=0.8, perform_grid_search=False, **kwargs):\n",
    "    '''\n",
    "    Returns predictions for the specified model by calling either generate_predictions_full or sarimax_full functions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pandas.core.frame.DataFrame\n",
    "              Pandas univariate dataframe to be used for predictions\n",
    "    \n",
    "    model_name : {'SARIMA', 'ETS', 'SARIMAX'} (Can accept in upper/lower/mixed case)\n",
    "                 SARIMA : Creates a SARIMA model\n",
    "                 ETS : Creates a triple Exponential Smoothing model\n",
    "                 SARIMAX : Creates a SARIMAX model\n",
    "    splits : float, tuple\n",
    "             Default = 0.8; splitting data in two sets - train and test of size 0.8 and 0.2 respectively\n",
    "             float : Splits in two sets - train and test\n",
    "                     Only accepts value between range (0.0, 1.0)\n",
    "             tuple(train_size, validation_size) : Splits in three sets - train, validation and test\n",
    "                                                  train_size -> Train size\n",
    "                                                  validation_size -> Validation size\n",
    "                                                  Should only contain values between range (0.0, 1.0) and \n",
    "                                                  \"train_size + test_size\" should also be between (0.0, 1.0)\n",
    "             Splits should be tuple if perform_grid_search is True\n",
    "    perform_grid_search : boolean\n",
    "                          Default = False; does not perform grid search\n",
    "    **kwargs : kwargs\n",
    "               Keyword arguments that can be used to provide parameters to the model or grid search of the specified model\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    val_preds : Predictions for val split (if splits is tuple)\n",
    "    test_preds : Predictions for test split\n",
    "    '''\n",
    "    model_name=model_name.upper()\n",
    "    if (model_name not in ['SARIMA', 'ETS', 'SARIMAX']):\n",
    "        raise ValueError(\"model_name must be one of {'SARIMA', 'ETS', 'SARIMAX'}\")\n",
    "        \n",
    "    if model_name in ['SARIMA', 'ETS']:\n",
    "        if perform_grid_search==False:\n",
    "            test_preds = generate_predictions_full(dataset, model_name, splits, perform_grid_search, **kwargs)\n",
    "            return test_preds\n",
    "        \n",
    "        elif perform_grid_search==True:\n",
    "            val_preds, test_preds = generate_predictions_full(dataset, model_name, splits, perform_grid_search, **kwargs)\n",
    "            return val_preds, test_preds\n",
    "        \n",
    "    elif model_name=='SARIMAX':\n",
    "        if perform_grid_search==False:\n",
    "            test_preds = sarimax_full(dataset, splits, perform_grid_search, **kwargs)\n",
    "            return test_preds\n",
    "        elif perform_grid_search==True:\n",
    "            val_preds, test_preds = sarimax_full(dataset, splits, perform_grid_search, **kwargs)\n",
    "            return val_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16aabe",
   "metadata": {
    "papermill": {
     "duration": 0.023139,
     "end_time": "2023-08-23T10:17:52.501832",
     "exception": false,
     "start_time": "2023-08-23T10:17:52.478693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Preparing data + Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07707cc",
   "metadata": {
    "papermill": {
     "duration": 0.022324,
     "end_time": "2023-08-23T10:17:52.547445",
     "exception": false,
     "start_time": "2023-08-23T10:17:52.525121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd169f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-23T10:17:52.594658Z",
     "iopub.status.busy": "2023-08-23T10:17:52.594050Z",
     "iopub.status.idle": "2023-08-23T10:17:53.769770Z",
     "shell.execute_reply": "2023-08-23T10:17:53.768018Z"
    },
    "papermill": {
     "duration": 1.201799,
     "end_time": "2023-08-23T10:17:53.771853",
     "exception": true,
     "start_time": "2023-08-23T10:17:52.570054",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Walmart_Store_sales.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWalmart_Store_sales.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Walmart_Store_sales.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Walmart_Store_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccbfff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94382e8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format=\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b4c797",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping columns to make data univariate\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_copy.drop(columns=['Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment'], inplace=True)\n",
    "\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ababe4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_of_df = {f'df_store_{i}' : df_copy.loc[df_copy.Store==i].set_index('Date') for i in df_copy['Store'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f126351",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_preds = df_copy.copy()\n",
    "df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66931f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_preds with 4 columns Store, date, weekly_sales, predictions(np.nan)\n",
    "\n",
    "df_preds['Predictions']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601e308",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_store_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1905b8c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### SARIMA [order=(2,0,2), seasonal_order=(1,0,0,52)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfaa8b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merged_df stores the final output generated by the model\n",
    "merged_df = pd.DataFrame(columns=['Store', 'Weekly_Sales', 'Date', 'Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d0f8e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fitting model for each store separately and generating predictions for SARIMAX [order=(2,0,2), seasonal_order(1,0,0,52)]\n",
    "\n",
    "for dataframe in dict_of_df.keys():\n",
    "    store_num = dataframe.split(sep='_')\n",
    "    store_num = store_num[-1]\n",
    "    store_num = int(store_num)\n",
    "    dict_of_df[dataframe].drop(columns='Store', inplace=True)\n",
    "    \n",
    "    test_preds = generate_predictions_full(dict_of_df[dataframe], \"SARIMA\", order=(2,0,2), seasonal_order=(1,0,0,52))\n",
    "    \n",
    "    predictions = pd.DataFrame(test_preds)\n",
    "    predictions.rename(columns={'predicted_mean':'Predictions'}, inplace=True)\n",
    "    predictions.reset_index(names='Date', inplace=True)\n",
    "    \n",
    "    predicted_store_df = pd.merge(df_preds.loc[df_preds['Store']==store_num], predictions, on=['Date'])\n",
    "    predicted_store_df.rename(columns={\"Predictions_y\":\"Predictions\"}, inplace=True)\n",
    "    predicted_store_df.drop(columns=\"Predictions_x\", inplace=True)\n",
    "    merged_df = pd.merge(merged_df, predicted_store_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009558f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sarima_total_eval = evaluation_metrics(merged_df['Weekly_Sales'], merged_df['Predictions'])\n",
    "sarima_total_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8c6b8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storing final predictions to csv\n",
    "\n",
    "# merged_df.to_csv(\"Walmart_Predictions.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9279f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### ETS [trend='add', seasonal='mul', seasonal_periods=52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ac33f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For storing final_predictions\n",
    "\n",
    "merged_df = pd.DataFrame(columns=['Store', 'Weekly_Sales', 'Date', 'Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb18e8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fitting model for each store separately and generating predictions for ETS [trend='add', seasonal='mul', seasonal_periods=52]\n",
    "# default smoothing_level, smoothing_trend, smoothing_seasonal\n",
    "\n",
    "for dataframe in dict_of_df.keys():\n",
    "    store_num = dataframe.split(sep='_')\n",
    "    store_num = store_num[-1]\n",
    "    store_num = int(store_num)\n",
    "    # dict_of_df[dataframe].drop(columns='Store', inplace=True)\n",
    "    \n",
    "    test_preds = generate_predictions_full(dict_of_df[dataframe], \"ets\",\n",
    "                                           trend='add', seasonal='mul', seasonal_periods=52)\n",
    "    \n",
    "    predictions = pd.DataFrame(test_preds)\n",
    "    predictions.rename(columns={'predicted_mean':'Predictions'}, inplace=True)\n",
    "    predictions.reset_index(names='Date', inplace=True)\n",
    "    \n",
    "    predicted_store_df = pd.merge(df_preds.loc[df_preds['Store']==store_num], predictions, on=['Date'])\n",
    "    predicted_store_df.rename(columns={\"Predictions_y\":\"Predictions\"}, inplace=True)\n",
    "    # predicted_store_df.drop(columns=\"Predictions_x\", inplace=True)\n",
    "    print(predicted_store_df)\n",
    "    merged_df = pd.merge(merged_df, predicted_store_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda8a43",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.drop(columns='Predictions', inplace=True)\n",
    "merged_df.rename(columns={0:\"Predictions\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6df036",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storing predictions to csv\n",
    "\n",
    "# merged_df.to_csv(\"Walmart_Predictions_ETS.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c24743",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d642e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ets_total_eval = evaluation_metrics(merged_df['Weekly_Sales'], merged_df['Predictions'])\n",
    "ets_total_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e1c55e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### ETS with grid search [trend=['add','mul'], seasonal=['add', 'mul'], seasonal_periods=[52], smoothing_values= [0.0, 0.1, 0.2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dc1b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_merged_df = pd.DataFrame(columns=['Store', 'Weekly_Sales', 'Date', 'Predictions'])\n",
    "test_merged_df = pd.DataFrame(columns=['Store', 'Weekly_Sales', 'Date', 'Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafacd41",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fitting model for each store separately and generating predictions for ETS using grid search\n",
    "# trend=['add','mul'], seasonal=['add','mul'], seasonal_periods=[52], smoothing_values=np.arange(0.0,0.3,0.1)\n",
    "\n",
    "# for dataframe in dict_of_df.keys():\n",
    "for dataframe in dict_of_df.keys():\n",
    "    store_num = dataframe.split(sep='_')\n",
    "    store_num = store_num[-1]\n",
    "    store_num = int(store_num)\n",
    "    # dict_of_df[dataframe].drop(columns='Store', inplace=True)\n",
    "    \n",
    "    val_preds, test_preds = generate_predictions_full(dict_of_df[dataframe], \"ets\", perform_grid_search=True, splits=(0.75, 0.125),\n",
    "                                                      trend=['add','mul'], seasonal=['add','mul'], seasonal_periods=[52],\n",
    "                                                      smoothing_values=np.arange(0.0,0.3,0.1))\n",
    "    \n",
    "    val_predictions = pd.DataFrame(val_preds)\n",
    "    val_predictions.rename(columns={'predicted_mean':'Predictions'}, inplace=True)\n",
    "    val_predictions.reset_index(names='Date', inplace=True)\n",
    "    val_predicted_store_df = pd.merge(df_preds.loc[df_preds['Store']==store_num], val_predictions, on=['Date'])\n",
    "    val_merged_df = pd.merge(val_merged_df, val_predicted_store_df, how='outer')\n",
    "    \n",
    "    test_predictions = pd.DataFrame(test_preds)\n",
    "    test_predictions.rename(columns={'predicted_mean':'Predictions'}, inplace=True)\n",
    "    test_predictions.reset_index(names='Date', inplace=True)\n",
    "    test_predicted_store_df = pd.merge(df_preds.loc[df_preds['Store']==store_num], test_predictions, on=['Date'])\n",
    "    test_merged_df = pd.merge(test_merged_df, test_predicted_store_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be39ed6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_merged_df.drop(columns=\"Predictions\", inplace=True)\n",
    "test_merged_df.drop(columns=\"Predictions\", inplace=True)\n",
    "\n",
    "val_merged_df.rename(columns={0:\"Predictions\"}, inplace=True)\n",
    "test_merged_df.rename(columns={0:\"Predictions\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3db695",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Storing predictions to csv\n",
    "\n",
    "# val_merged_df.to_csv(\"Val_Grid_ETS.csv\", encoding='utf-8')\n",
    "# test_merged_df.to_csv(\"Test_Grid_ETS.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5e0f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ets_grid_total_eval = evaluation_metrics(val_merged_df['Weekly_Sales'], val_merged_df['Predictions'])\n",
    "ets_grid_total_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1b1d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### SARIMA with grid search [p_range=range(0, 2), d_range=range(0, 1), q_range=range(0, 2), P_range=range(0, 2), D_range=range(0, 1), Q_range=range(0, 2), m=[52]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a75a55",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_merged_df = pd.DataFrame(columns=['Store', 'Weekly_Sales', 'Date', 'Predictions'])\n",
    "test_merged_df = pd.DataFrame(columns=['Store', 'Weekly_Sales', 'Date', 'Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d308cf2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_range=range(0, 2)\n",
    "d_range=range(0, 1)\n",
    "q_range=range(0, 2)\n",
    "P_range=range(0, 2)\n",
    "D_range=range(0, 1)\n",
    "Q_range=range(0, 2)\n",
    "m=[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed328ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Fitting model for each store separately and generating predictions for SARIMA using grid search\n",
    "# # p_range=range(0, 2), d_range=range(0, 1), q_range=range(0, 2), \n",
    "# # P_range=range(0, 2), D_range=range(0, 1), Q_range=range(0, 2), m=[52]\n",
    "\n",
    "for dataframe in dict_of_df.keys():\n",
    "    store_num = dataframe.split(sep='_')\n",
    "    store_num = store_num[-1]\n",
    "    store_num = int(store_num)\n",
    "    # dict_of_df[dataframe].drop(columns='Store', inplace=True)\n",
    "    \n",
    "    val_preds, test_preds = generate_predictions_full(dict_of_df[dataframe], model_name=\"sarima\", splits=(0.75,0.125), perform_grid_search=True, \n",
    "                                                      p_range=p_range, d_range=d_range, q_range=q_range,\n",
    "                                                      P_range=P_range, D_range=D_range, Q_range=Q_range, m=m)\n",
    "    \n",
    "    val_predictions = pd.DataFrame(val_preds)\n",
    "    val_predictions.rename(columns={'predicted_mean':'Predictions'}, inplace=True)\n",
    "    val_predictions.reset_index(names='Date', inplace=True)\n",
    "    val_predicted_store_df = pd.merge(df_preds.loc[df_preds['Store']==store_num], val_predictions, on=['Date'])\n",
    "    val_merged_df = pd.merge(val_merged_df, val_predicted_store_df, how='outer')\n",
    "    \n",
    "    test_predictions = pd.DataFrame(test_preds)\n",
    "    test_predictions.rename(columns={'predicted_mean':'Predictions'}, inplace=True)\n",
    "    test_predictions.reset_index(names='Date', inplace=True)\n",
    "    test_predicted_store_df = pd.merge(df_preds.loc[df_preds['Store']==store_num], test_predictions, on=['Date'])\n",
    "    test_merged_df = pd.merge(test_merged_df, test_predicted_store_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ae87a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_merged_df.drop(columns=['Predictions_x', 'Predictions'], inplace=True)\n",
    "test_merged_df.drop(columns=['Predictions_x', 'Predictions'], inplace=True)\n",
    "\n",
    "val_merged_df.rename(columns={\"Predictions_y\": \"Predictions\"}, inplace=True)\n",
    "test_merged_df.rename(columns={\"Predictions_y\": \"Predictions\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcf2b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Storing predictions to csv\n",
    "\n",
    "val_merged_df.to_csv(\"Val_Grid_SARIMA.csv\", encoding='utf-8')\n",
    "test_merged_df.to_csv(\"Test_Grid_SARIMA.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d5281",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sarima_grid_total_eval = evaluation_metrics(val_merged_df['Weekly_Sales'], val_merged_df['Predictions'])\n",
    "sarima_grid_total_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad3f77",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### SARIMA Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad36e0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49560ddf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_agg = df.resample('W').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624ed37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = df_agg[:int(0.8*(len(df_agg)))]\n",
    "test = df_agg[int(0.8*(len(df_agg))):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e822eb4c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.drop(columns='Store', inplace=True)\n",
    "test.drop(columns='Store', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53279704",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_y = train['Weekly_Sales']\n",
    "train_x = train.drop(columns='Weekly_Sales')\n",
    "\n",
    "test_y = test['Weekly_Sales']\n",
    "test_x = test.drop(columns='Weekly_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2f2db",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_preds, test_preds = sarimax_model(train_x, test_x, train_y, test_y, order=(2,0,0), seasonal_order=(1,0,0,52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9724d0d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_plot(\"SARIMAX\", train_y, test_y, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab8e6f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_of_df2 = {f'df_store_{i}' : df.loc[df.Store==i] for i in df_copy['Store'].unique()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b0f355",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### SARIMAX (order=(2,0,2), seasonal_order=(1,0,0,52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0b073",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame(columns=['Store', 'Weekly_Sales', 'Date', 'Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab1073",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataframe in dict_of_df2.keys():\n",
    "    store_num = dataframe.split(sep='_')\n",
    "    store_num = store_num[-1]\n",
    "    store_num = int(store_num)\n",
    "    dict_of_df2[dataframe].drop(columns='Store', inplace=True)\n",
    "    test_preds = sarimax_full(dict_of_df2[dataframe], order=(2,0,2), seasonal_order=(1,0,0,52))\n",
    "    predictions = pd.DataFrame(test_preds)\n",
    "    predictions.rename(columns={'predicted_mean':'Predictions'}, inplace=True)\n",
    "    predictions.reset_index(names='Date', inplace=True)\n",
    "    predicted_store_df = pd.merge(df_preds.loc[df_preds['Store']==store_num], predictions, on=['Date'], how='outer')\n",
    "    predicted_store_df.rename(columns={\"Predictions_y\":\"Predictions\"}, inplace=True)\n",
    "    predicted_store_df.drop(columns=\"Predictions_x\", inplace=True)\n",
    "    merged_df = pd.merge(merged_df, predicted_store_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783b306",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merged_df.to_csv(\"SARIMAX.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099baf32",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcb7af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sarimax_total_eval = evaluation_metrics(merged_df['Weekly_Sales'], merged_df['Predictions'])\n",
    "sarimax_total_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39604961",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### SARIMAX with grid search [p_range = range(0,2), d_range = range(0,1), q_range = range(0,2), P_range = range(0,2), D_range = range(0,1), Q_range = range(0,2), m=[52]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e3870",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_range = range(0,2)\n",
    "d_range = range(0,1)\n",
    "q_range = range(0,2)\n",
    "P_range = range(0,2)\n",
    "D_range = range(0,1)\n",
    "Q_range = range(0,2)\n",
    "m=[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e7c60",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_merged_df = pd.DataFrame(columns=['Store', 'Weekly_Sales', 'Date', 'Predictions'])\n",
    "test_merged_df = pd.DataFrame(columns=['Store', 'Weekly_Sales', 'Date', 'Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d680696",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fitting model for each store separately and generating predictions for SARIMA using grid search\n",
    "# p_range=range(0, 2), d_range=range(0, 1), q_range=range(0, 2), \n",
    "# P_range=range(0, 2), D_range=range(0, 1), Q_range=range(0, 2), m=[52]\n",
    "\n",
    "for dataframe in dict_of_df2.keys():\n",
    "    store_num = dataframe.split(sep='_')\n",
    "    store_num = store_num[-1]\n",
    "    store_num = int(store_num)\n",
    "    # dict_of_df2[dataframe].drop(columns='Store', inplace=True)\n",
    "    \n",
    "    val_preds, test_preds = sarimax_full(dict_of_df2[dataframe], splits=(0.75,0.125), perform_grid_search=True, \n",
    "                                         p_range=p_range, d_range=d_range, q_range=q_range,\n",
    "                                         P_range=P_range, D_range=D_range, Q_range=Q_range, m=m)\n",
    "    \n",
    "    val_predictions = pd.DataFrame(val_preds)\n",
    "    val_predictions.rename(columns={'predicted_mean':'Predictions'}, inplace=True)\n",
    "    val_predictions.reset_index(names='Date', inplace=True)\n",
    "    val_predicted_store_df = pd.merge(df_preds.loc[df_preds['Store']==store_num], val_predictions, on=['Date'])\n",
    "    val_merged_df = pd.merge(val_merged_df, val_predicted_store_df, how='outer')\n",
    "    \n",
    "    test_predictions = pd.DataFrame(test_preds)\n",
    "    test_predictions.rename(columns={'predicted_mean':'Predictions'}, inplace=True)\n",
    "    test_predictions.reset_index(names='Date', inplace=True)\n",
    "    test_predicted_store_df = pd.merge(df_preds.loc[df_preds['Store']==store_num], test_predictions, on=['Date'])\n",
    "    test_merged_df = pd.merge(test_merged_df, test_predicted_store_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d0c24",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_merged_df.drop(columns=['Predictions', 'Predictions_x'], inplace=True)\n",
    "test_merged_df.drop(columns=['Predictions', 'Predictions_x'], inplace=True)\n",
    "\n",
    "val_merged_df.rename(columns={'Predictions_y':'Predictions'}, inplace=True)\n",
    "test_merged_df.rename(columns={'Predictions_y':'Predictions'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9ca11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sarimax_grid_total_eval = evaluation_metrics(val_merged_df['Weekly_Sales'], val_merged_df['Predictions'])\n",
    "sarimax_grid_total_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339eb3c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val_merged_df.to_csv(\"Val_Grid_SARIMAX.csv\", encoding='utf-8')\n",
    "# test_merged_df.to_csv(\"Test_Grid_SARIMAX.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae8de5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sarima_total_eval)\n",
    "print(ets_total_eval)\n",
    "print(ets_grid_total_eval)\n",
    "print(sarima_grid_total_eval)\n",
    "print(sarimax_total_eval)\n",
    "print(sarimax_grid_total_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f582cce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Grid Search sklearn GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765df99",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sarima_params_grid = {'order':[(p,d,q) for p in range(0,2) for d in range(0,1) for q in range(0,2)],\n",
    "                      'seasonal_order':[(P,D,Q,m) for P in range(0,2) for D in range(0,1) for Q in range(0,2) for m in [12]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a905b9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SARIMAWrapper(BaseEstimator):\n",
    "    def __init__(self, order, seasonal_order):\n",
    "        self.order = order\n",
    "        self.seasonal_order = seasonal_order\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = SARIMAX(y, order=self.order, seasonal_order=self.seasonal_order)\n",
    "        self.model_fit = self.model.fit()\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model_fit.forecast(steps=len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7cdaef",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sarima_wrapper = SARIMAWrapper(order=(1,0,0), seasonal_order=(1,0,0,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464812c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "sarima_grid_search = GridSearchCV(sarima_wrapper, sarimax_params_grid, scoring=make_scorer(evaluation_metrics), \n",
    "                                  cv=TimeSeriesSplit(n_splits=5), error_score='raise', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec7db8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.229589,
   "end_time": "2023-08-23T10:17:54.920642",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-23T10:17:36.691053",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
